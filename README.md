# ArtiFish
Toolkit for GenAI in cybersecurity. Details, resources, and discussions can be found at [ArtiFish.dev](https://ArtiFish.dev).

## Background
ArtiFish is a toolkit designed to enable low-cost fine-tuning of new skills, with the ability to reference both external data (e.g., the Internet) and internal references and documents. This toolkit represents the culmination of that work to date.

## Model Support and Selection
ArtiFish heavily focuses on the use of Unsloth-optimized versions of the Meta Llama 3 Instruct family of models. While the approaches outlined here can be applied to other variants, most research to date has been conducted using these models.

### Unsloth AI
[Unsloth AI](https://github.com/unslothai/unsloth) publishes models to Huggingface at [Unsloth Models](https://huggingface.co/unsloth). These models are optimized for use with Unsloth libraries. These optimizations enable 4-bit training and inference that are nearly as effective as more hardware-intensive 8-bit and 16-bit quantizations. This allows fine-tuning of models on commodity GPUs at a fraction of the time and cost of other trainers. All fine-tuning in this library currently utilizes the innovations and simplifications created by Unsloth AI.

The `FastLanguageModel` from Unsloth replaces the Huggingface Transformers equivalent of `AutoModelForCausalLM`. In most use cases, `FastLanguageModel` is preferred for its resource efficiency and accuracy. It is also easier to launch and maintain stability. `FastLanguageModel` simplifies the process of adding weights to models, especially when interacting with Unsloth-optimized models. Examples of `Transformer`-based chat implementations can be found in the `chatbots` directory if preferred.

### Meta Llama 3 Instruct Models
While many of the approaches in this library can be applied to other models, the provided code is deeply focused on utilizing Llama 3. Its balance of support across other packages and its stable matrix make it a good fit. Note that accessing these models from Huggingface requires accepting their terms. It is recommended to use the Unsloth-optimized models.

### Recommended Models
The current recommended model for most workloads is [unsloth/Llama-3.2-1B-Instruct-unsloth-bnb-4bit](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-unsloth-bnb-4bit). It has a minimal GPU memory requirement for inference, is extremely accurate and flexible, and is easy to fine-tune. Using the `chatbots/rag-and-web-enabled.py` chatbot, it requires less than 4GB of GPU RAM, can quickly query the Internet (via the SERP API), and process documents in Chroma generated by `RAG/create_database.py`. Larger versions of this model, such as the `3B` and `11B` parameter variants, are available if the additional resource cost is justified.

---

## Installation of ArtiFish
1. Install Python 3.10, preferably using Conda (see below).
2. Install CUDA Toolkit 12.4: [Download CUDA Toolkit 12.4](https://developer.nvidia.com/cuda-12-4-0-download-archive).
3. Install RAG dependencies (see below) if extracting documents for local lookup.
4. Clone or download the repository to a local directory, then navigate to the directory.
5. Run `pip install -r requirements.txt` to install dependencies.
6. Create a `settings.yaml` file by copying `settings.yaml.example.yaml` and editing it.
7. Configure and run the appropriate script.

### Conda Installation
Conda environments enable Python packages to be configured with specific versions and dependencies that are difficult to manage without Conda.

### Conda Installation in Windows
- Download and install [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html)
- Run Miniconda shell (not powershell!) as Administrator
- `conda create -n artifish -y`
- `conda activate artifish`
- `conda install python=3.10 -c conda-forge -y`
- `python --version`  # should say python 3.10.xx
### Conda Installation on Mac
- Download and Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html#macos-installers) for Python 3.10.
- `conda create -n artifish -y`
- `conda activate artifish`
- `conda install python=3.10 -c conda-forge -y`
- `python --version`  # should say python 3.10.xx

## RAG Dependencies
Doing document extraction requires external applications to assist in create vector chunks for lookup
## External applications for RAG
- Pandoc https://pandoc.org/installing.html. Pandoc is a versatile, open-source document converter that allows users to convert files from one markup format to another.
- LibreOffice https://www.libreoffice.org/download/download-libreoffice/. LibreOffice is an open-source equivalent of MS Office. It provides features for extracting those file types
- Tesseract https://github.com/tesseract-ocr/tessdoc. Tesseract allows optical character recognition.
- Poppler https://poppler.freedesktop.org/. Poppler allows for extraction of PDF data.

All packages above are available for MAC, Ubuntu and Windows.
Note:: When installing to Windows update the PATH environment to the binary folder of each of these packages

# Scripts
The code in this repo are divided into 3 sub-folders: `dataset-generation`, `fine-tuning`, `RAG` and `chatbots`

## dataset-generation
The files in the `dataset-creation` folder were used to create public facing and private datasts used by WitFoo R&D
- `precinct-to-syslog-dataset.py` - Connects to a WitFoo Precinct Cassandra cluster, analyzes Incidents and Artifacts to create a dataset for training how to translate syslog formats to English.
- `syslog-to-parser-code.py` - Used to take input syslog messages and code examples to create parsers for training a codegen model.
- Public datasets can be found on WitFoo's Huggingface page at https://huggingface.co/witfoo.

## fine-tuning
The files in the `fine-tuning` folder are used to train a model to understand a dataset
- `unsloth-llama3-fine-tune-from-CSV.py` - Fine tuning script that loads a local model and CSV file and writes a newly tuned model.
- `unsloth-llama3-fine-tune-from-dataset.py` - Fine tuning script that loads a local or HF model and HF Dataset and writes a newly tuned model.
- `download-model.py` - Download a Huggingface model to local disk.
- Public fine tuned models that used this script can be found on WitFoo's Huggingface page at https://huggingface.co/witfoo.

## chatbots
The files in the `chatbots` folder create Web User Interfaces to interact with models.
- `cassandra-rag-and-web-enabled.py` - Chatbot that searches Web (via SERPAPI) and referencing RAG local data in Cassandra (see RAG below)
- `chroma-rag-and-web-enabled.py` - Chatbot that searches Web (via SERPAPI) and referencing RAG local data in ChromaDB (see RAG below)
- `unsloth-llama3-instruct-chatbot.py` - Chatbot for interacting with an Unsloth optimized Llama 3 Instruct model
- `huggingface-llama3-instruct-chatbot.py` - Chatbot using HF standard transformers
- `local-model-unsloth-chatbot.py` - Chatbot for interacting with a local, fine-tuned model with Unsloth optimizations. Can run on small GPU (CPU not supported)
- `local-model-transformers-chatbot.py` - Chatbot for interacting with a local, fine-tuned model with standard Transformers. Can run on GPU or CPU.
- `witq-unsloth-chatbot.py` - Chatbot for interacting with WitFoo's Opensource model with Unsloth optimizations. Can run on small GPU (CPU not supported)
- `witq-transformers-chatbot.py` - Chatbot for interacting with WitFoo's Opensource model with standard Transformers. Can run on GPU or CPU.
- Public fine tuned models that used this script can be found on WitFoo's Huggingface page at https://huggingface.co/witfoo.

## RAG
The files in the `RAG` folder are used to make local documents searchable by the model using Retrieval Augmented Generation (RAG). 
- `create_database_chroma.py` - recursively search a directory of documents and create a Chroma vector database for use in RAG lookups by the models.
- `create_database_cassandra.py` - recursively search a directory of documents and create a Cassandra vector database for use in RAG lookups by the models.
